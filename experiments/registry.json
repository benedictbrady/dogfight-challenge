{
  "next_id": 3,
  "experiments": [
    {
      "id": 1,
      "date": "2025-02-18",
      "tag": "baseline-v1",
      "config": {
        "n_envs": 16,
        "n_steps": 10800,
        "action_repeat": 1,
        "num_updates": 300,
        "lr": 3e-4,
        "ent_coef": 0.003,
        "gamma": 0.99,
        "curriculum": "do_nothing+dogfighter(0-100) -> dogfighter+chaser(100-250) -> chaser+ace(250+)"
      },
      "status": "completed",
      "notes": "First training run. Entropy drifted upward, win rate stagnated ~18%. Sparse rewards with full-match rollouts made credit assignment hard.",
      "results": {
        "final_win_rate": 0.19,
        "final_return": -0.34
      }
    },
    {
      "id": 2,
      "date": "2026-02-19",
      "tag": "unified-dr-v1",
      "config_file": "experiments/configs/unified_dr_v1.json",
      "config": {
        "script": "train_unified_dr",
        "model": "768h/4b, obs_dim=59 (46 base + 13 config)",
        "n_envs": 256,
        "n_steps": 2048,
        "action_repeat": 10,
        "phases": "curriculum(600) + transition(200) + selfplay(1500) = 2300 total",
        "domain_randomization": "progressive (narrowâ†’full at update 200)",
        "gpu": "A10G",
        "actor_params": "~4.78M (48% of 10M budget)"
      },
      "status": "pending",
      "notes": "First config-aware DR training run. 768h/4b model (5.2x larger than old 384h/3b). Sees physics config in observation. 8 named eval regimes. A10G GPU for ~2.5h estimated runtime."
    }
  ]
}
